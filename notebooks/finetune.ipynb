{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23e0d093-e4b8-494a-93a5-4923736a6960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2953bf1a-72c8-412d-a3e5-6cc93b6bf333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.dataset import load_datasets \n",
    "from notebooks.model import get_model  \n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "461132e8-b545-4e17-8d43-5fe5276f36c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_loader, val_loader, test_loader = load_datasets(\"pedestrian_risk_analysis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "876372fe-ac04-4d75-a493-2142311392dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/nhansen3/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/packages/miniconda-t2/20230523/envs/jupyter-cuda121-20230610/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/packages/miniconda-t2/20230523/envs/jupyter-cuda121-20230610/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiUAAAGiCAYAAAA4MLYWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAkVklEQVR4nO3df3TU9b3n8dfAJEOIySwJMMNIwFiz/gpQDTYl9RYsEC4LUtduQbFePNK9Ij8uc4HDDzm7xN42QXoK2uVKV8oBBb3pHxpLT1EIW4lyOfRiMGuCHopLKkEzzVXjJLFxAuGzfyDf6xBAk0yYz0yej3O+p+T7/czweUOtz34zM3EZY4wAAADibEC8NwAAACARJQAAwBJECQAAsAJRAgAArECUAAAAKxAlAADACkQJAACwAlECAACsQJQAAAArECUAAMAKcY2Sp59+Wrm5uRo0aJAKCgr0xhtvxHM7AAAgjuIWJb/5zW8UDAa1du1avfXWW/qbv/kbTZ8+XadOnYrXlgAAQBy54vUD+QoLC3X77bdry5Ytzrmbb75Z99xzj8rKyuKxJQAAEEfuePymHR0dqq6u1urVq6POFxcX69ChQ13WRyIRRSIR5+tz587pk08+UXZ2tlwuV5/vFwAA9IwxRq2trQoEAhow4MrfoIlLlHz00Ufq7OyUz+eLOu/z+RQKhbqsLysr0+OPP361tgcAAGKsoaFBI0eOvOKauETJBRff5TDGXPLOx5o1a7Rs2TLn63A4rFGjRkn6R0mePt4lAADouYikTcrIyPjKlXGJkqFDh2rgwIFd7oo0NTV1uXsiSR6PRx7PpeLDI6IEAAD7fZ2XW8Tl3TepqakqKChQZWVl1PnKykoVFRXFY0sAACDO4vbtm2XLlunBBx/U+PHjNWHCBD3zzDM6deqUFixYEK8tAQCAOIpblMyZM0cff/yxfvKTn6ixsVH5+fnas2ePRo8eHa8tAQCAOIrb55T0RktLi7xer6TV4jUlAADYLCJpvcLhsDIzM6+4kp99AwAArECUAAAAKxAlAADACkQJAACwAlECAACsQJQAAAArECUAAMAKRAkAALACUQIAAKxAlAAAACsQJQAAwApECQAAsAJRAgAArECUAAAAKxAlAADACkQJAACwAlECAACsQJQAAAArECUAAMAKRAkAALACUQIAAKxAlAAAACsQJQAAwApECQAAsAJRAgAArECUAAAAKxAlAADACkQJAACwAlECAACsQJQAAAArECUAAMAKRAkAALACUQIAAKxAlAAAACsQJQAAwApECQAAsAJRAgAArECUAAAAKxAlAADACkQJAACwAlECAACsQJQAAAArECUAAMAKRAkAALACUQIAAKxAlAAAACsQJQAAwApECQAAsAJRAgAArECUAAAAKxAlAADACt2Oktdff1133323AoGAXC6XXn755ajrxhiVlJQoEAgoLS1NkyZN0rFjx6LWRCIRLVmyREOHDlV6erpmzZql06dP92oQAACQ2LodJZ999pnGjRunzZs3X/L6hg0btHHjRm3evFlHjhyR3+/X1KlT1dra6qwJBoOqqKhQeXm5Dh48qLa2Ns2cOVOdnZ09nwQAACQ0lzHG9PjBLpcqKip0zz33SDp/lyQQCCgYDGrVqlWSzt8V8fl8euKJJ/TII48oHA5r2LBh2rlzp+bMmSNJ+vDDD5WTk6M9e/Zo2rRpX/n7trS0yOv1SlotydPT7QMAgD4XkbRe4XBYmZmZV1wZ09eU1NfXKxQKqbi42Dnn8Xg0ceJEHTp0SJJUXV2tM2fORK0JBALKz8931lwsEomopaUl6gAAAMklplESCoUkST6fL+q8z+dzroVCIaWmpmrIkCGXXXOxsrIyeb1e58jJyYnltgEAgAX65N03Lpcr6mtjTJdzF7vSmjVr1igcDjtHQ0NDzPYKAADsENMo8fv9ktTljkdTU5Nz98Tv96ujo0PNzc2XXXMxj8ejzMzMqAMAACSXmEZJbm6u/H6/KisrnXMdHR2qqqpSUVGRJKmgoEApKSlRaxobG1VXV+esAQAA/Y+7uw9oa2vTe++953xdX1+vmpoaZWVladSoUQoGgyotLVVeXp7y8vJUWlqqwYMHa+7cuZIkr9er+fPna/ny5crOzlZWVpZWrFihMWPGaMqUKbGbDAAAJJRuR8mbb76pu+66y/l62bJlkqR58+Zpx44dWrlypdrb27Vw4UI1NzersLBQ+/btU0ZGhvOYTZs2ye12a/bs2Wpvb9fkyZO1Y8cODRw4MAYjAQCARNSrzymJFz6nBACARBGnzykBAADoKaIEAABYgSgBAABWIEoAAIAViBIAAGAFogQAAFiBKAEAAFYgSgAAgBWIEgAAYAWiBAAAWIEoAQAAViBKAACAFYgSAABgBaIEAABYgSgBAABWIEoAAIAViBIAAGAFogQAAFiBKAEAAFYgSgAAgBWIEgAAYAWiBAAAWIEoAQAAViBKAACAFYgSAABgBaIEAABYgSgBAABWIEoAAIAViBIAAGAFogQAAFiBKAEAAFYgSgAAgBWIEgAAYAWiBAAAWIEoAQAAViBKAACAFYgSAABgBaIEAABYgSgBAABWIEoAAIAViBIAAGAFogQAAFiBKAEAAFYgSgAAgBWIEgAAYAWiBAAAWIEoAQAAViBKAACAFYgSAABgBaIEAABYoVtRUlZWpjvuuEMZGRkaPny47rnnHh0/fjxqjTFGJSUlCgQCSktL06RJk3Ts2LGoNZFIREuWLNHQoUOVnp6uWbNm6fTp072fBgAAJKxuRUlVVZUWLVqkw4cPq7KyUmfPnlVxcbE+++wzZ82GDRu0ceNGbd68WUeOHJHf79fUqVPV2trqrAkGg6qoqFB5ebkOHjyotrY2zZw5U52dnbGbDAAAJBSXMcb09MH//u//ruHDh6uqqkrf/e53ZYxRIBBQMBjUqlWrJJ2/K+Lz+fTEE0/okUceUTgc1rBhw7Rz507NmTNHkvThhx8qJydHe/bs0bRp077y921paZHX65W0WpKnp9sHAAB9LiJpvcLhsDIzM6+4slevKQmHw5KkrKwsSVJ9fb1CoZCKi4udNR6PRxMnTtShQ4ckSdXV1Tpz5kzUmkAgoPz8fGdNl3EiEbW0tEQdAAAgufQ4SowxWrZsme68807l5+dLkkKhkCTJ5/NFrfX5fM61UCik1NRUDRky5LJrLlZWViav1+scOTk5Pd02AACwVI+jZPHixXr77bf1L//yL12uuVyuqK+NMV3OXexKa9asWaNwOOwcDQ0NPd02AACwVI+iZMmSJdq9e7dee+01jRw50jnv9/slqcsdj6amJufuid/vV0dHh5qbmy+75mIej0eZmZlRBwAASC7dihJjjBYvXqyXXnpJf/jDH5Sbmxt1PTc3V36/X5WVlc65jo4OVVVVqaioSJJUUFCglJSUqDWNjY2qq6tz1gAAgP7H3Z3FixYt0gsvvKDf/va3ysjIcO6IeL1epaWlyeVyKRgMqrS0VHl5ecrLy1NpaakGDx6suXPnOmvnz5+v5cuXKzs7W1lZWVqxYoXGjBmjKVOmxH5CAACQELoVJVu2bJEkTZo0Ker89u3b9dBDD0mSVq5cqfb2di1cuFDNzc0qLCzUvn37lJGR4azftGmT3G63Zs+erfb2dk2ePFk7duzQwIEDezcNAABIWL36nJJ44XNKAABIFFfpc0oAAABihSgBAABWIEoAAIAViBIAAGAFogQAAFiBKAEAAFYgSgAAgBWIEgAAYAWiBAAAWIEoAQAAViBKAACAFYgSAABgBaIEAABYgSgBAABWIEoAAIAViBIAAGAFogQAAFiBKAEAAFYgSgAAgBWIEgAAYAWiBAAAWIEoAQAAViBKAACAFYgSAABgBaIEAABYgSgBAABWIEoAAIAViBIAAGAFogQAAFiBKAEAAFYgSgAAgBWIEgAAYAWiBAAAWIEoAQAAViBKAACAFYgSAABgBaIEAABYgSgBAABWIEoAAIAViBIAAGAFogQAAFiBKAEAAFYgSgAAgBWIEgAAYAWiBAAAWIEoAQAAViBKAACAFYgSAABgBaIEAABYgSgBAABWIEoAAIAVuhUlW7Zs0dixY5WZmanMzExNmDBBr7zyinPdGKOSkhIFAgGlpaVp0qRJOnbsWNRzRCIRLVmyREOHDlV6erpmzZql06dPx2YaAACQsLoVJSNHjtT69ev15ptv6s0339T3vvc9ff/733fCY8OGDdq4caM2b96sI0eOyO/3a+rUqWptbXWeIxgMqqKiQuXl5Tp48KDa2to0c+ZMdXZ2xnYyAACQUFzGGNObJ8jKytLPf/5zPfzwwwoEAgoGg1q1apWk83dFfD6fnnjiCT3yyCMKh8MaNmyYdu7cqTlz5kiSPvzwQ+Xk5GjPnj2aNm3a1/o9W1pa5PV6Ja2W5OnN9gEAQJ+KSFqvcDiszMzMK67s8WtKOjs7VV5ers8++0wTJkxQfX29QqGQiouLnTUej0cTJ07UoUOHJEnV1dU6c+ZM1JpAIKD8/HxnzSXHiUTU0tISdQAAgOTS7Sipra3VNddcI4/HowULFqiiokK33HKLQqGQJMnn80Wt9/l8zrVQKKTU1FQNGTLksmsupaysTF6v1zlycnK6u20AAGC5bkfJjTfeqJqaGh0+fFiPPvqo5s2bp3feece57nK5otYbY7qcu9hXrVmzZo3C4bBzNDQ0dHfbAADAct2OktTUVN1www0aP368ysrKNG7cOD311FPy+/2S1OWOR1NTk3P3xO/3q6OjQ83NzZddcykej8d5x8+FAwAAJJdef06JMUaRSES5ubny+/2qrKx0rnV0dKiqqkpFRUWSpIKCAqWkpEStaWxsVF1dnbMGAAD0T+7uLH7sscc0ffp05eTkqLW1VeXl5Tpw4IBeffVVuVwuBYNBlZaWKi8vT3l5eSotLdXgwYM1d+5cSZLX69X8+fO1fPlyZWdnKysrSytWrNCYMWM0ZcqUPhkQAAAkhm5FyV/+8hc9+OCDamxslNfr1dixY/Xqq69q6tSpkqSVK1eqvb1dCxcuVHNzswoLC7Vv3z5lZGQ4z7Fp0ya53W7Nnj1b7e3tmjx5snbs2KGBAwfGdjIAAJBQev05JfHA55QAAJAorsLnlAAAAMQSUQIAAKxAlCDBZUpKi/cmAAAx0K0XugJ2uVnmv94nDZdcE4z00C5J78V7UwCAHiJKkLC+Zfza4pL+Imnd/3bpRvN9zXX9XNLz8d4aAKAH+PYNEtSj+uNPJukvXzpzyvVb/V/9Z+nJknhtCgDQC0QJElOdTyXrok+1S3pJ0rkHXZLGxGFTAIDeIEqQkMzjl/8Bjo9nS5ryg6u3GQBATBAlSDwjS6R/vfKScZWHr8pWAACxQ5QgwWSqtOEfVfLhlVfV/PcJV2c7AICY4d03SDAPaITLrz/HexsAgJjjTgkSSrW56+sFSY0krfuKRQAAmxAlSByLS3Rb9rtfa2nJm9L9ZruklL7dEwAgZogSJIhMmQMuPf7J13/E37vmS7q7z3YEAIgtogQJ4Dr9k/lQW+q696h/lVRj/pukwr7YFAAgxogS2O+mh+Ry/SLq01u/jjOSKlwnVGcWi2/jAID9iBJYz/zKpTO9eLzXdVL65tqY7QcA0DeIEtivpncP/7UkI5eUXxKDzQAA+gpRgn6hpEYy//PyH00PAIg/ogT9xm9mSzPNrZLS4r0VAMAlECWwW7BEp4Oxeap3Jc1yzVZ722rpppLYPCkAIGaIEljNuz6kX8fw+T6QtP4ayQziNSYAYBuiBFYLD2pVyUN98MQeSXXdfZMxAKAvESWw3PPSN2P/rD8+/L8kbYn9EwMAeowoQb9TkidtS1sc720AAC5ClMB6rjtNzJ4rTdJTf/p76fOSmD0nACA23PHeAPCV/vb8h8T35lNdL7hb0q2uGZKOxuDZAACxxJ0S9Cv/aF6WVBvvbQAALoEogf0+OqC1B3v/NCXF0r6U7ys291wAALFGlCABHNDG7zza62f5H3vXSGdLer8dAECfIEqQEJanPK0f9+LxaZJ+mlIaq+0AAPoAUYLEcHajRub2/OGrXpJ0NgbfAwIA9BnefYME0SLXG+16emSaHt0qadB/XKl+UPrdZR71qCTfTuk//ZdGSb/q+20CAHqMKEHiGDlIPzH/TwszrpfOful8nZR/65FLPuTx/3OHNFPSg9VXZYsAgJ4jSpBAfqNQ2pyup8dLdbrjKx57nc6/sqQ99tsCAMQErylBArm75w91Z+t8lAAAbEWUIIEMjvcGAAB9iChBgrg2Bs+RF4PnAAD0FaIECeL2qHfcdJtbkibq/E/RAQDYiChBAkiTdHPvn2bQYEm39/55AAB9gihBArjhi6CIhenibgkA2IkogeVSdD4kAADJjiiB5W6X3LzrBgD6A6IEFkuR9Lcx/oi/M18cAADbECWw2LcktyvGz/lKjJ8PABArRAksda3knsoPQgCAfoQogaWmxz5IPv9Y0nsxflIAQKzw/0NhqV5+guuXf4rw2XpJTZIOiB/IBwD2IkqQXM5KOtsiabekT7442Spe3AoA9iNKkFzOvi3ppXjvAgDQA716TUlZWZlcLpeCwaBzzhijkpISBQIBpaWladKkSTp27FjU4yKRiJYsWaKhQ4cqPT1ds2bN0unTp3uzFUD6XJL+GO9dAAB6qMdRcuTIET3zzDMaO3Zs1PkNGzZo48aN2rx5s44cOSK/36+pU6eqtbXVWRMMBlVRUaHy8nIdPHhQbW1tmjlzpjo7O3s+Cfq3zyVph6QP4rsPAECP9ShK2tra9MADD2jr1q0aMmSIc94YoyeffFJr167Vvffeq/z8fD377LP661//qhdeeEGSFA6HtW3bNv3iF7/QlClTdNttt2nXrl2qra3V/v37YzMV+hcnSP4c120AAHqnR1GyaNEizZgxQ1OmTIk6X19fr1AopOLiYuecx+PRxIkTdejQIUlSdXW1zpw5E7UmEAgoPz/fWXOxSCSilpaWqAP4D6dFkABA4uv2C13Ly8t19OhRHTlypMu1UCgkSfL5fFHnfT6f3n//fWdNampq1B2WC2suPP5iZWVlevzxx7u7VSS7s5LOnpb0fLx3AgCIgW7dKWloaNDSpUu1a9cuDRo06LLrXK7ojwY3xnQ5d7ErrVmzZo3C4bBzNDQ0dGfbSFZn/yLpWfHZIwCQHLoVJdXV1WpqalJBQYHcbrfcbreqqqr0y1/+Um6327lDcvEdj6amJuea3+9XR0eHmpubL7vmYh6PR5mZmVEHkt3z0uct0R+CdsHnkj6vl/Rr8fkjAJA8uhUlkydPVm1trWpqapxj/PjxeuCBB1RTU6Prr79efr9flZWVzmM6OjpUVVWloqIiSVJBQYFSUlKi1jQ2Nqqurs5ZA5z/OPiN0tnd0ucHvwgRfelFrc+KIAGA5NKt15RkZGQoPz8/6lx6erqys7Od88FgUKWlpcrLy1NeXp5KS0s1ePBgzZ07V5Lk9Xo1f/58LV++XNnZ2crKytKKFSs0ZsyYLi+cBaSjX/xn1ZfOESMAkIxi/omuK1euVHt7uxYuXKjm5mYVFhZq3759ysjIcNZs2rRJbrdbs2fPVnt7uyZPnqwdO3Zo4MCBsd4OkgYhAgDJzmWMMfHeRHe1tLTI6/VKWi3JE+/tAACAy4pIWq9wOPyVrwnt1cfMAwAAxApRAgAArECUAAAAKxAlAADACkQJAACwAlECAACsQJQAAAArECUAAMAKRAkAALACUQIAAKxAlAAAACsQJQAAwApECQAAsAJRAgAArECUAAAAKxAlAADACkQJAACwAlECAACsQJQAAAArECUAAMAKRAkAALACUQIAAKxAlAAAACsQJQAAwApECQAAsAJRAgAArECUAAAAKxAlAADACkQJAACwAlECAACsQJQAAAArECUAAMAKRAkAALACUQIAAKxAlAAAACsQJQAAwApECQAAsAJRAgAArECUAAAAKxAlAADACkQJAACwAlECAACsQJQAAAArECUAAMAKRAkAALACUQIAAKxAlAAAACsQJQAAwApECQAAsAJRAgAArECUAAAAK3QrSkpKSuRyuaIOv9/vXDfGqKSkRIFAQGlpaZo0aZKOHTsW9RyRSERLlizR0KFDlZ6erlmzZun06dOxmQYAACSsbt8pufXWW9XY2OgctbW1zrUNGzZo48aN2rx5s44cOSK/36+pU6eqtbXVWRMMBlVRUaHy8nIdPHhQbW1tmjlzpjo7O2MzEQAASEjubj/A7Y66O3KBMUZPPvmk1q5dq3vvvVeS9Oyzz8rn8+mFF17QI488onA4rG3btmnnzp2aMmWKJGnXrl3KycnR/v37NW3atF6OAwAAElW375ScOHFCgUBAubm5uu+++3Ty5ElJUn19vUKhkIqLi521Ho9HEydO1KFDhyRJ1dXVOnPmTNSaQCCg/Px8Z82lRCIRtbS0RB0AACC5dCtKCgsL9dxzz2nv3r3aunWrQqGQioqK9PHHHysUCkmSfD5f1GN8Pp9zLRQKKTU1VUOGDLnsmkspKyuT1+t1jpycnO5sGwAAJIBuRcn06dP1gx/8QGPGjNGUKVP0+9//XtL5b9Nc4HK5oh5jjOly7mJftWbNmjUKh8PO0dDQ0J1tAwCABNCrtwSnp6drzJgxOnHihPM6k4vveDQ1NTl3T/x+vzo6OtTc3HzZNZfi8XiUmZkZdQAAgOTSqyiJRCJ69913NWLECOXm5srv96uystK53tHRoaqqKhUVFUmSCgoKlJKSErWmsbFRdXV1zhoAANA/devdNytWrNDdd9+tUaNGqampST/96U/V0tKiefPmyeVyKRgMqrS0VHl5ecrLy1NpaakGDx6suXPnSpK8Xq/mz5+v5cuXKzs7W1lZWVqxYoXz7SAAANB/dStKTp8+rfvvv18fffSRhg0bpm9/+9s6fPiwRo8eLUlauXKl2tvbtXDhQjU3N6uwsFD79u1TRkaG8xybNm2S2+3W7Nmz1d7ersmTJ2vHjh0aOHBgbCcDAAAJxWWMMfHeRHe1tLTI6/VKWi3JE+/tAACAy4pIWq9wOPyVrwnlZ98AAAArECUAAMAKRAkAALACUQIAAKxAlAAAACsQJQAAwApECQAAsAJRAgAArECUAAAAKxAlAADACkQJAACwAlECAACsQJQAAAArECUAAMAKRAkAALACUQIAAKxAlAAAACsQJQAAwApECQAAsAJRAgAArECUAAAAKxAlAADACkQJAACwAlECAACsQJQAAAArECUAAMAKRAkAALACUQIAAKxAlAAAACsQJQAAwApECQAAsAJRAgAArECUAAAAKxAlAADACkQJAACwAlECAACsQJQAAAArECUAAMAKRAkAALACUQIAAKxAlAAAACsQJQAAwApECQAAsAJRAgAArECUAAAAKxAlAADACkQJAACwAlECAACsQJQAAAArECUAAMAKRAkAALACUQIAAKxAlAAAACu4472BnjDGfPGrSFz3AQAAvsr5f1f/x7+7Ly8ho6S1tfWLX22K6z4AAMDX09raKq/Xe8U1LvN10sUy586d0/Hjx3XLLbeooaFBmZmZ8d7SVdHS0qKcnJx+NbPUP+fujzNLzN2f5u6PM0v9c25jjFpbWxUIBDRgwJVfNZKQd0oGDBiga6+9VpKUmZnZb/5iL+iPM0v9c+7+OLPE3P1Jf5xZ6n9zf9Udkgt4oSsAALACUQIAAKyQsFHi8Xi0bt06eTyeeG/lqumPM0v9c+7+OLPE3P1p7v44s9R/5/66EvKFrgAAIPkk7J0SAACQXIgSAABgBaIEAABYgSgBAABWIEoAAIAVEjJKnn76aeXm5mrQoEEqKCjQG2+8Ee8t9crrr7+uu+++W4FAQC6XSy+//HLUdWOMSkpKFAgElJaWpkmTJunYsWNRayKRiJYsWaKhQ4cqPT1ds2bN0unTp6/iFN1TVlamO+64QxkZGRo+fLjuueceHT9+PGpNss29ZcsWjR071vkkxwkTJuiVV15xrifbvJdSVlYml8ulYDDonEvGuUtKSuRyuaIOv9/vXE/GmS/44IMP9KMf/UjZ2dkaPHiwvvnNb6q6utq5noyzX3fddV3+vl0ulxYtWiQpOWfuMybBlJeXm5SUFLN161bzzjvvmKVLl5r09HTz/vvvx3trPbZnzx6zdu1a8+KLLxpJpqKiIur6+vXrTUZGhnnxxRdNbW2tmTNnjhkxYoRpaWlx1ixYsMBce+21prKy0hw9etTcddddZty4cebs2bNXeZqvZ9q0aWb79u2mrq7O1NTUmBkzZphRo0aZtrY2Z02yzb17927z+9//3hw/ftwcP37cPPbYYyYlJcXU1dUZY5Jv3ov927/9m7nuuuvM2LFjzdKlS53zyTj3unXrzK233moaGxudo6mpybmejDMbY8wnn3xiRo8ebR566CHzxz/+0dTX15v9+/eb9957z1mTjLM3NTVF/V1XVlYaSea1114zxiTnzH0l4aLkW9/6llmwYEHUuZtuusmsXr06TjuKrYuj5Ny5c8bv95v169c75z7//HPj9XrNr371K2OMMZ9++qlJSUkx5eXlzpoPPvjADBgwwLz66qtXbe+90dTUZCSZqqoqY0z/mXvIkCHm17/+ddLP29raavLy8kxlZaWZOHGiEyXJOve6devMuHHjLnktWWc2xphVq1aZO++887LXk3n2L1u6dKn5xje+Yc6dO9dvZo6VhPr2TUdHh6qrq1VcXBx1vri4WIcOHYrTrvpWfX29QqFQ1Mwej0cTJ050Zq6urtaZM2ei1gQCAeXn5yfMn0s4HJYkZWVlSUr+uTs7O1VeXq7PPvtMEyZMSPp5Fy1apBkzZmjKlClR55N57hMnTigQCCg3N1f33XefTp48KSm5Z969e7fGjx+vH/7whxo+fLhuu+02bd261bmezLNf0NHRoV27dunhhx+Wy+XqFzPHUkJFyUcffaTOzk75fL6o8z6fT6FQKE676lsX5rrSzKFQSKmpqRoyZMhl19jMGKNly5bpzjvvVH5+vqTknbu2tlbXXHONPB6PFixYoIqKCt1yyy1JO68klZeX6+jRoyorK+tyLVnnLiws1HPPPae9e/dq69atCoVCKioq0scff5y0M0vSyZMntWXLFuXl5Wnv3r1asGCB/uEf/kHPPfecpOT9+/6yl19+WZ9++qkeeughSf1j5lhyx3sDPeFyuaK+NsZ0OZdsejJzovy5LF68WG+//bYOHjzY5VqyzX3jjTeqpqZGn376qV588UXNmzdPVVVVzvVkm7ehoUFLly7Vvn37NGjQoMuuS7a5p0+f7vx6zJgxmjBhgr7xjW/o2Wef1be//W1JyTezJJ07d07jx49XaWmpJOm2227TsWPHtGXLFv3d3/2dsy4ZZ79g27Ztmj59ugKBQNT5ZJ45lhLqTsnQoUM1cODALuXY1NTUpUKTxYVX7F9pZr/fr46ODjU3N192ja2WLFmi3bt367XXXtPIkSOd88k6d2pqqm644QaNHz9eZWVlGjdunJ566qmknbe6ulpNTU0qKCiQ2+2W2+1WVVWVfvnLX8rtdjv7Tra5L5aenq4xY8boxIkTSft3LUkjRozQLbfcEnXu5ptv1qlTpyQl7z/XF7z//vvav3+/fvzjHzvnkn3mWEuoKElNTVVBQYEqKyujzldWVqqoqChOu+pbubm58vv9UTN3dHSoqqrKmbmgoEApKSlRaxobG1VXV2ftn4sxRosXL9ZLL72kP/zhD8rNzY26nqxzX8wYo0gkkrTzTp48WbW1taqpqXGO8ePH64EHHlBNTY2uv/76pJz7YpFIRO+++65GjBiRtH/XkvSd73yny1v7//SnP2n06NGSkv+f6+3bt2v48OGaMWOGcy7ZZ465q/3K2t668Jbgbdu2mXfeeccEg0GTnp5u/vznP8d7az3W2tpq3nrrLfPWW28ZSWbjxo3mrbfect7mvH79euP1es1LL71kamtrzf3333/Jt5ONHDnS7N+/3xw9etR873vfs/rtZI8++qjxer3mwIEDUW+l++tf/+qsSba516xZY15//XVTX19v3n77bfPYY4+ZAQMGmH379hljkm/ey/nyu2+MSc65ly9fbg4cOGBOnjxpDh8+bGbOnGkyMjKc/51KxpmNOf+2b7fbbX72s5+ZEydOmOeff94MHjzY7Nq1y1mTrLN3dnaaUaNGmVWrVnW5lqwz94WEixJjjPnnf/5nM3r0aJOammpuv/12522kieq1114zkroc8+bNM8acfxvdunXrjN/vNx6Px3z3u981tbW1Uc/R3t5uFi9ebLKyskxaWpqZOXOmOXXqVBym+XouNa8ks337dmdNss398MMPO/+9HTZsmJk8ebITJMYk37yXc3GUJOPcFz6HIiUlxQQCAXPvvfeaY8eOOdeTceYLfve735n8/Hzj8XjMTTfdZJ555pmo68k6+969e40kc/z48S7XknXmvuAyxpi43KIBAAD4koR6TQkAAEheRAkAALACUQIAAKxAlAAAACsQJQAAwApECQAAsAJRAgAArECUAAAAKxAlAADACkQJAACwAlECAACs8P8BIZudb5duaSoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = torch.hub.load(\"pytorch/vision:v0.10.0\", \"deeplabv3_resnet101\", pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "image = Image.open(\"DAVID-sim/m1596437/Images/Video_002/v002_0002.png\")\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "image_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(image_tensor)[\"out\"][0]  # Get segmentation map\n",
    "\n",
    "output_predictions = output.argmax(0)\n",
    "\n",
    "plt.imshow(output_predictions, cmap=\"jet\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c5abff6-1ab5-46d5-b027-3ed61f1ca22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "\n",
    "model = models.segmentation.deeplabv3_resnet101(pretrained=True)\n",
    "\n",
    "in_channels = model.classifier[4].in_channels \n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.AdaptiveAvgPool2d((2, 2)),  \n",
    "    nn.Flatten(),  \n",
    "    nn.Linear(in_channels, 3)  \n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48968515-e2d0-4052-b9b2-6f7a400c19fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "352b568b-a3de-4069-af82-127f4a1f546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0a146c6-3eb7-4465-97f0-391d7c96de72",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 9.50 GiB of which 10.94 MiB is free. Process 3096096 has 110.00 MiB memory in use. Process 3118724 has 610.00 MiB memory in use. Including non-PyTorch memory, this process has 8.87 GiB memory in use. Of the allocated memory 8.76 GiB is allocated by PyTorch, and 16.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m     val_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m correct \u001b[38;5;241m/\u001b[39m total\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(val_loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, device, num_epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/packages/miniconda-t2/20230523/envs/jupyter-cuda121-20230610/lib/python3.8/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/packages/miniconda-t2/20230523/envs/jupyter-cuda121-20230610/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/packages/miniconda-t2/20230523/envs/jupyter-cuda121-20230610/lib/python3.8/site-packages/torchvision/models/segmentation/_utils.py:23\u001b[0m, in \u001b[0;36m_SimpleSegmentationModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# contract: features is a dict of tensors\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m result \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/packages/miniconda-t2/20230523/envs/jupyter-cuda121-20230610/lib/python3.8/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/packages/miniconda-t2/20230523/envs/jupyter-cuda121-20230610/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/packages/miniconda-t2/20230523/envs/jupyter-cuda121-20230610/lib/python3.8/site-packages/torchvision/models/_utils.py:69\u001b[0m, in \u001b[0;36mIntermediateLayerGetter.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m out \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 69\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_layers:\n\u001b[1;32m     71\u001b[0m         out_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_layers[name]\n",
      "File \u001b[0;32m/packages/miniconda-t2/20230523/envs/jupyter-cuda121-20230610/lib/python3.8/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/packages/miniconda-t2/20230523/envs/jupyter-cuda121-20230610/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/packages/miniconda-t2/20230523/envs/jupyter-cuda121-20230610/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/packages/miniconda-t2/20230523/envs/jupyter-cuda121-20230610/lib/python3.8/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/packages/miniconda-t2/20230523/envs/jupyter-cuda121-20230610/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/packages/miniconda-t2/20230523/envs/jupyter-cuda121-20230610/lib/python3.8/site-packages/torchvision/models/resnet.py:151\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    148\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[1;32m    150\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out)\n\u001b[0;32m--> 151\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[1;32m    154\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(out)\n",
      "File \u001b[0;32m/packages/miniconda-t2/20230523/envs/jupyter-cuda121-20230610/lib/python3.8/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/packages/miniconda-t2/20230523/envs/jupyter-cuda121-20230610/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/packages/miniconda-t2/20230523/envs/jupyter-cuda121-20230610/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/packages/miniconda-t2/20230523/envs/jupyter-cuda121-20230610/lib/python3.8/site-packages/torch/nn/functional.py:2483\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2481\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2483\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2484\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2485\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 9.50 GiB of which 10.94 MiB is free. Process 3096096 has 110.00 MiB memory in use. Process 3118724 has 610.00 MiB memory in use. Including non-PyTorch memory, this process has 8.87 GiB memory in use. Of the allocated memory 8.76 GiB is allocated by PyTorch, and 16.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "def train(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)[\"out\"]  \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_acc = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "        validate(model, val_loader, criterion, device)\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)[\"out\"]\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    val_acc = 100 * correct / total\n",
    "    print(f\"Validation Loss: {val_loss/len(val_loader):.4f}, Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "train(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d003cb58-c1c3-47a1-924a-685ebed02503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
